{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. Unggah File CSV\n",
        "print(\"Silakan unggah file 'BMKG Jateng - Sheet1.csv'\")\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "file_name = 'BMKG Jateng - Sheet1.csv'\n",
        "print(f\"\\nFile '{file_name}' berhasil diunggah.\")"
      ],
      "metadata": {
        "id": "Cc0n1GZdhAAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "="
      ],
      "metadata": {
        "id": "tsu6mFIIhGwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Muat dan Pembersihan Awal Data ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import files\n",
        "\n",
        "print(\"\\n--- Memulai Muat dan Pembersihan Awal Data (Bagian 2) ---\")\n",
        "try:\n",
        "    df_raw = pd.read_csv(file_name, decimal=',', na_values=['-',''])\n",
        "    print(\"\\nData mentah berhasil dimuat.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saat memuat data: {e}\")\n",
        "    # df_raw = pd.read_csv(file_name, decimal=',', na_values=['-',''], encoding='latin1')\n",
        "\n",
        "# Simpan salinan data mentah sebelum modifikasi untuk inspeksi Bagian 2\n",
        "df_bagian2_output_raw = df_raw.copy()\n",
        "\n",
        "# Tampilkan beberapa baris pertama data mentah\n",
        "print(\"\\nBeberapa baris pertama data mentah (awal Bagian 2):\")\n",
        "print(df_raw.head())\n",
        "print(\"\\nTipe data awal (awal Bagian 2):\")\n",
        "print(df_raw.info())"
      ],
      "metadata": {
        "id": "1Xf0fR6mhGAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Konversi kolom TANGGAL ke datetime\n",
        "try:\n",
        "    df_raw['TANGGAL'] = pd.to_datetime(df_raw['TANGGAL'], format='%d-%m-%Y')\n",
        "    print(\"\\nKolom TANGGAL berhasil dikonversi ke datetime.\")\n",
        "except ValueError as e:\n",
        "    print(f\"Error konversi tanggal: {e}. Periksa format tanggal di CSV.\")\n",
        "\n",
        "# Set TANGGAL sebagai index\n",
        "df_raw.set_index('TANGGAL', inplace=True)\n",
        "\n",
        "# Drop kolom DDD_CAR karena tidak akan diprediksi/digunakan\n",
        "if 'DDD_CAR' in df_raw.columns:\n",
        "    df_raw.drop('DDD_CAR', axis=1, inplace=True)\n",
        "    print(\"\\nKolom 'DDD_CAR' telah dihapus.\")\n",
        "\n",
        "# Konversi kolom numerik lainnya\n",
        "numeric_cols = ['TN', 'TX', 'TAVG', 'RH_AVG', 'RR', 'SS', 'FF_X', 'DDD_X', 'FF_AVG']\n",
        "for col in numeric_cols:\n",
        "    if col in df_raw.columns:\n",
        "        if df_raw[col].dtype == 'object':\n",
        "            df_raw[col] = df_raw[col].str.strip()\n",
        "            df_raw[col] = df_raw[col].str.replace(',', '.', regex=False)\n",
        "        df_raw[col] = pd.to_numeric(df_raw[col], errors='coerce')\n",
        "\n",
        "print(\"\\nTipe data setelah konversi numerik (akhir Bagian 2):\")\n",
        "print(df_raw.info())"
      ],
      "metadata": {
        "id": "kEFmXBPGlXmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nData setelah pembersihan awal (akhir Bagian 2):\")\n",
        "print(df_raw.head())"
      ],
      "metadata": {
        "id": "0atMhiMGlRYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ekspor dan Unduh Data Hasil Bagian 2\n",
        "df_bagian2_output_cleaned = df_raw.copy() # df_raw sudah dimodifikasi\n",
        "csv_filename_bagian2_raw = 'hasil_bagian2_data_mentah_awal.csv'\n",
        "csv_filename_bagian2_cleaned = 'hasil_bagian2_data_setelah_pembersihan_awal.csv'\n",
        "\n",
        "print(f\"\\nMengekspor data mentah awal Bagian 2 ke {csv_filename_bagian2_raw}...\")\n",
        "df_bagian2_output_raw.to_csv(csv_filename_bagian2_raw, index=False) # index=False karena TANGGAL belum jadi index\n",
        "files.download(csv_filename_bagian2_raw)\n",
        "print(f\"File {csv_filename_bagian2_raw} siap diunduh.\")\n",
        "\n",
        "print(f\"\\nMengekspor data setelah pembersihan awal Bagian 2 ke {csv_filename_bagian2_cleaned}...\")\n",
        "df_bagian2_output_cleaned.to_csv(csv_filename_bagian2_cleaned, index=True) # index=True karena TANGGAL sudah jadi index\n",
        "files.download(csv_filename_bagian2_cleaned)\n",
        "print(f\"File {csv_filename_bagian2_cleaned} siap diunduh.\")\n",
        "\n",
        "print(\"\\n--- Muat dan Pembersihan Awal Data (Bagian 2) Selesai ---\")"
      ],
      "metadata": {
        "id": "m5UlzFLohJrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "="
      ],
      "metadata": {
        "id": "nZ-bpAfthM9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Exploratory Data Analysis (EDA) ---\n",
        "print(\"\\n--- Memulai Exploratory Data Analysis (EDA) (Bagian 3) ---\")\n",
        "\n",
        "# df_eda_input adalah df_raw yang sudah dibersihkan dari Bagian 2\n",
        "# Kita buat salinan untuk memastikan df_raw tidak termodifikasi jika ada operasi tak sengaja\n",
        "df_eda_input = df_raw.copy()\n",
        "primary_blue_color = 'royalblue'\n",
        "\n",
        "# A. Statistik Deskriptif\n",
        "print(\"\\nStatistik Deskriptif (EDA):\")\n",
        "print(df_eda_input.describe())"
      ],
      "metadata": {
        "id": "-mm3zl0KhZ3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Asumsikan df_eda_input sudah ada dan diindeks berdasarkan tanggal\n",
        "\n",
        "# B. Cek dan Visualisasi Nilai Hilang\n",
        "print(\"\\nJumlah nilai hilang per kolom (EDA):\")\n",
        "missing_values_eda = df_eda_input.isnull().sum()\n",
        "print(missing_values_eda[missing_values_eda > 0]) # Tampilkan hanya kolom dengan nilai hilang\n",
        "\n",
        "if df_eda_input.isnull().sum().sum() > 0:\n",
        "    plt.figure(figsize=(14, 8)) # Mungkin perlu ukuran lebih besar\n",
        "\n",
        "    # Membuat matriks boolean True/False untuk nilai hilang\n",
        "    missing_matrix = df_eda_input.isnull()\n",
        "\n",
        "    ax = sns.heatmap(missing_matrix, cbar=False, cmap='viridis', yticklabels=False) # yticklabels=False agar tidak terlalu ramai\n",
        "    plt.title('Heatmap Nilai Hilang dalam Data Input EDA (dengan Penanda Tahun)')\n",
        "    plt.xlabel('Nama Kolom Fitur')\n",
        "    plt.ylabel('Sampel Data (Diurutkan Berdasarkan Tanggal)')\n",
        "\n",
        "    # Menambahkan penanda tahun pada sumbu Y\n",
        "    # Dapatkan tahun unik dan posisi baris pertama setiap tahun\n",
        "    unique_years = df_eda_input.index.year.unique()\n",
        "    year_positions = []\n",
        "    year_labels = []\n",
        "\n",
        "    for year in unique_years:\n",
        "        # Temukan indeks baris pertama untuk tahun tersebut di missing_matrix\n",
        "        # Karena missing_matrix memiliki index yang sama dengan df_eda_input\n",
        "        try:\n",
        "            # Dapatkan indeks numerik dari baris pertama tahun tersebut\n",
        "            first_occurrence_index = missing_matrix.index.get_loc(missing_matrix[missing_matrix.index.year == year].index[0])\n",
        "            year_positions.append(first_occurrence_index)\n",
        "            year_labels.append(str(year))\n",
        "        except IndexError:\n",
        "            # Mungkin tahun tersebut tidak memiliki data (jarang terjadi jika df_eda_input padat)\n",
        "            pass\n",
        "        except KeyError:\n",
        "            # Jika ada masalah dengan get_loc (jarang terjadi dengan index datetime yang standar)\n",
        "             print(f\"Peringatan: Tidak bisa menemukan posisi untuk tahun {year}\")\n",
        "\n",
        "    # Jika jumlah tahun terlalu banyak, kita bisa mengambil sampel tahun\n",
        "    if len(year_positions) > 15: # Batasi jumlah label tahun agar tidak terlalu ramai\n",
        "        step = len(year_positions) // 15\n",
        "        year_positions = year_positions[::step]\n",
        "        year_labels = year_labels[::step]\n",
        "\n",
        "    ax.set_yticks(year_positions)\n",
        "    ax.set_yticklabels(year_labels, rotation=0) # rotation=0 agar horizontal\n",
        "    ax.tick_params(axis='y', which='major', labelsize=8) # Sesuaikan ukuran font jika perlu\n",
        "\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nTidak ada nilai hilang dalam dataset input untuk EDA.\")"
      ],
      "metadata": {
        "id": "JJIQuu7tjQg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C. Visualisasi Time Series untuk Variabel Cuaca Utama\n",
        "print(\"\\nVisualisasi Time Series Variabel Cuaca Utama (EDA)...\")\n",
        "main_weather_vars_eda = ['TX', 'TN', 'TAVG', 'RH_AVG', 'RR', 'SS']\n",
        "plt.figure(figsize=(18, 15))\n",
        "for i, var in enumerate(main_weather_vars_eda):\n",
        "    if var in df_eda_input.columns:\n",
        "        plt.subplot(len(main_weather_vars_eda), 1, i + 1)\n",
        "        # Menggunakan warna biru yang konsisten untuk semua time series\n",
        "        df_eda_input[var].plot(label=var, color=primary_blue_color, alpha=0.8)\n",
        "        plt.title(f'Time Series of {var}')\n",
        "        plt.ylabel(var)\n",
        "        plt.xlabel('Tanggal')\n",
        "        plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XlCJdWjRjKJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# D. Visualisasi Distribusi Fitur Numerik\n",
        "print(\"\\nVisualisasi Distribusi Fitur Numerik (EDA)...\")\n",
        "numeric_features_for_dist_eda = df_eda_input.select_dtypes(include=np.number).columns\n",
        "num_plots_dist = len(numeric_features_for_dist_eda)\n",
        "cols_dist = 3\n",
        "rows_dist = (num_plots_dist + cols_dist - 1) // cols_dist\n",
        "\n",
        "plt.figure(figsize=(15, rows_dist * 4))\n",
        "for i, col in enumerate(numeric_features_for_dist_eda):\n",
        "    plt.subplot(rows_dist, cols_dist, i + 1)\n",
        "    # Menggunakan warna biru untuk histogram\n",
        "    sns.histplot(df_eda_input[col].dropna(), kde=True, color=primary_blue_color, line_kws={'color': 'darkblue'}) # KDE line bisa warna lebih gelap\n",
        "    plt.title(f'Distribusi {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frekuensi')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mbLGJc0OjIIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# E. Matriks Korelasi\n",
        "print(\"\\nMatriks Korelasi Fitur Numerik (EDA)...\")\n",
        "numeric_df_for_corr = df_eda_input.select_dtypes(include=np.number)\n",
        "correlation_matrix_eda = numeric_df_for_corr.corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "# cmap 'coolwarm' adalah standar yang baik untuk korelasi (merah-biru)\n",
        "# Jika ingin dominan biru, bisa gunakan cmap='Blues' atau cmap='PuBu' (Purple-Blue)\n",
        "# atau cmap='GnBu' (Green-Blue)\n",
        "sns.heatmap(correlation_matrix_eda, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5, annot_kws={\"size\": 8})\n",
        "# Jika ingin mencoba palet biru:\n",
        "# sns.heatmap(correlation_matrix_eda, annot=True, cmap='Blues', fmt=\".2f\", linewidths=.5, annot_kws={\"size\": 8})\n",
        "plt.title('Matriks Korelasi Fitur Numerik (EDA)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BUqPP53xjF0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# F. Box Plots untuk melihat pola musiman (Contoh: TX per bulan)\n",
        "print(\"\\nBox Plot TX per Bulan (EDA)...\")\n",
        "if 'TX' in df_eda_input.columns:\n",
        "    # Buat kolom bulan sementara jika belum ada (dari index)\n",
        "    df_eda_input['month_for_plot'] = df_eda_input.index.month\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.boxplot(x='month_for_plot', y='TX', data=df_eda_input, palette=\"Set3\")\n",
        "    plt.title('Box Plot Suhu Maksimum (TX) per Bulan')\n",
        "    plt.xlabel('Bulan')\n",
        "    plt.ylabel('TX (°C)')\n",
        "    plt.xticks(ticks=range(0,12), labels=['Jan','Feb','Mar','Apr','Mei','Jun','Jul','Ags','Sep','Okt','Nov','Des'])\n",
        "    plt.show()\n",
        "    # Hapus kolom sementara\n",
        "    df_eda_input.drop('month_for_plot', axis=1, inplace=True, errors='ignore') # errors='ignore' jika sudah dihapus\n",
        "else:\n",
        "    print(\"Kolom 'TX' tidak ditemukan untuk Box Plot.\")\n",
        "\n",
        "# (Anda bisa menambahkan plot EDA lain di sini jika diperlukan, misalnya scatter plot antar variabel tertentu)"
      ],
      "metadata": {
        "id": "fK6U3Q-pjEJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# G. Scatter Plot (Contoh: TX vs TAVG)\n",
        "print(\"\\nScatter Plot TX vs TAVG (EDA)...\")\n",
        "if 'TX' in df_eda_input.columns and 'TAVG' in df_eda_input.columns:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    # Menggunakan warna biru untuk scatter plot\n",
        "    sns.scatterplot(x='TAVG', y='TX', data=df_eda_input, alpha=0.6, color=primary_blue_color)\n",
        "    plt.title('Scatter Plot TX vs TAVG')\n",
        "    plt.xlabel('Temperatur Rata-rata (TAVG °C)')\n",
        "    plt.ylabel('Temperatur Maksimum (TX °C)')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Kolom 'TX' atau 'TAVG' tidak ditemukan untuk Scatter Plot.\")"
      ],
      "metadata": {
        "id": "P32IPrjAjCyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Ekspor dan Unduh Data yang Digunakan untuk EDA (Hasil dari Bagian 2) ---\n",
        "# Ini adalah df_eda_input, yang merupakan salinan dari df_raw setelah pembersihan awal.\n",
        "\n",
        "csv_filename_bagian3 = 'hasil_bagian3_data_input_untuk_eda.csv'\n",
        "print(f\"\\nMengekspor data input yang digunakan untuk EDA ke {csv_filename_bagian3}...\")\n",
        "\n",
        "# Pastikan index (Tanggal) diekspor\n",
        "df_eda_input.to_csv(csv_filename_bagian3, index=True)\n",
        "files.download(csv_filename_bagian3)\n",
        "print(f\"File {csv_filename_bagian3} siap diunduh.\")\n",
        "\n",
        "print(\"\\n--- Exploratory Data Analysis (EDA) (Bagian 3) Selesai ---\")"
      ],
      "metadata": {
        "id": "oy76N_CGh-8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "="
      ],
      "metadata": {
        "id": "VXNB0fG4hYw-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtKVKgOIgrBK"
      },
      "outputs": [],
      "source": [
        "# --- 4. Data Preprocessing ---\n",
        "print(\"\\n--- Memulai Data Preprocessing (Bagian 4) ---\")\n",
        "# df_processed akan menjadi output dari Bagian 4\n",
        "df_for_preprocessing = df_raw.copy() # Mulai dari data bersih Bagian 2\n",
        "\n",
        "# Penanganan Nilai Hilang (Interpolasi)\n",
        "print(\"\\nJumlah nilai hilang SEBELUM interpolasi (Bagian 4):\")\n",
        "print(df_for_preprocessing.isnull().sum()[df_for_preprocessing.isnull().sum() > 0])\n",
        "df_processed = df_for_preprocessing.copy()\n",
        "for col in df_processed.select_dtypes(include=np.number).columns:\n",
        "    df_processed[col] = df_processed[col].interpolate(method='linear', limit_direction='both')\n",
        "print(\"\\nJumlah nilai hilang SETELAH interpolasi (Bagian 4):\")\n",
        "missing_after_interpolation = df_processed.isnull().sum()\n",
        "print(missing_after_interpolation[missing_after_interpolation > 0])\n",
        "if missing_after_interpolation.sum() == 0:\n",
        "    print(\"Semua nilai hilang numerik telah ditangani dengan interpolasi (Bagian 4).\")\n",
        "else:\n",
        "    df_processed.dropna(inplace=True) # Langkah akhir jika masih ada\n",
        "    print(\"Nilai hilang yang tersisa telah di-drop (Bagian 4).\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering\n",
        "print(\"\\nMelakukan Feature Engineering (Bagian 4)...\")\n",
        "df_processed['Tx_target'] = df_processed['TX'].shift(-1)\n",
        "lags_to_create = [1, 2, 3]\n",
        "weather_features_for_lag = ['TN', 'TX', 'TAVG', 'RH_AVG', 'RR', 'SS', 'FF_X', 'DDD_X', 'FF_AVG']\n",
        "for feature in weather_features_for_lag:\n",
        "    if feature in df_processed.columns:\n",
        "        for lag in lags_to_create:\n",
        "            df_processed[f'{feature}_lag{lag}'] = df_processed[feature].shift(lag)\n",
        "df_processed['dayofweek'] = df_processed.index.dayofweek\n",
        "df_processed['dayofyear'] = df_processed.index.dayofyear\n",
        "df_processed['month'] = df_processed.index.month\n",
        "df_processed['year'] = df_processed.index.year\n",
        "\n",
        "# Menghapus baris dengan NaN yang dihasilkan oleh shift\n",
        "rows_before_dropna_fe = len(df_processed)\n",
        "df_processed.dropna(inplace=True)\n",
        "rows_after_dropna_fe = len(df_processed)\n",
        "print(f\"\\nMenghapus baris dengan NaN hasil dari shift (Bagian 4). Jumlah baris berkurang dari {rows_before_dropna_fe} menjadi {rows_after_dropna_fe}.\")\n",
        "\n",
        "print(\"\\nData setelah Preprocessing (akhir Bagian 4):\")\n",
        "print(df_processed.head())"
      ],
      "metadata": {
        "id": "T8VF3FWDhlte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ekspor dan Unduh Data Hasil Bagian 4\n",
        "csv_filename_bagian4 = 'hasil_bagian4_data_setelah_preprocessing.csv'\n",
        "print(f\"\\nMengekspor data setelah preprocessing ke {csv_filename_bagian4}...\")\n",
        "df_processed.to_csv(csv_filename_bagian4, index=True)\n",
        "files.download(csv_filename_bagian4)\n",
        "print(f\"File {csv_filename_bagian4} siap diunduh.\")\n",
        "\n",
        "print(\"\\n--- Data Preprocessing (Bagian 4) Selesai ---\")"
      ],
      "metadata": {
        "id": "u4Lrn1w2hjao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "="
      ],
      "metadata": {
        "id": "HtrXeD6pheHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Pembagian Data (Splitting Data secara kronologis untuk time series) ---\n",
        "print(\"\\n--- Memulai Pembagian Data (Splitting Data) (Bagian 5) ---\")\n",
        "\n",
        "if 'Tx_target' not in df_processed.columns:\n",
        "    print(\"Kolom 'Tx_target' tidak ditemukan. Pastikan Feature Engineering telah dilakukan.\")\n",
        "else:\n",
        "    X = df_processed.drop('Tx_target', axis=1)\n",
        "    y = df_processed['Tx_target']\n",
        "    train_size_percentage = 0.8\n",
        "    train_samples = int(len(df_processed) * train_size_percentage)\n",
        "    X_train = X.iloc[:train_samples]\n",
        "    y_train = y.iloc[:train_samples]\n",
        "    X_test = X.iloc[train_samples:]\n",
        "    y_test = y.iloc[train_samples:]\n",
        "\n",
        "    print(f\"\\nJumlah total sampel setelah preprocessing: {len(df_processed)}\")\n",
        "    print(f\"Jumlah sampel training (X_train, y_train): {len(X_train)}\")\n",
        "    print(f\"Jumlah sampel testing (X_test, y_test): {len(X_test)}\")\n",
        "\n",
        "print(\"\\n--- Pembagian Data (Bagian 5) Selesai ---\")\n",
        "print(\"Variabel X_train, y_train, X_test, y_test siap digunakan dan telah diekspor.\")"
      ],
      "metadata": {
        "id": "4RUBRDVehelM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ekspor dan Unduh Data Hasil Bagian 5\n",
        "# Kita akan mengekspor X_train, y_train, X_test, y_test secara terpisah\n",
        "csv_filename_X_train = 'hasil_bagian5_X_train.csv'\n",
        "csv_filename_y_train = 'hasil_bagian5_y_train.csv'\n",
        "csv_filename_X_test = 'hasil_bagian5_X_test.csv'\n",
        "csv_filename_y_test = 'hasil_bagian5_y_test.csv'\n",
        "\n",
        "print(f\"\\nMengekspor X_train ke {csv_filename_X_train}...\")\n",
        "X_train.to_csv(csv_filename_X_train, index=True)\n",
        "files.download(csv_filename_X_train)\n",
        "print(f\"File {csv_filename_X_train} siap diunduh.\")\n",
        "\n",
        "print(f\"\\nMengekspor y_train ke {csv_filename_y_train}...\")\n",
        "y_train.to_csv(csv_filename_y_train, index=True, header=['Tx_target']) # Tambah header jika y_train adalah Series\n",
        "files.download(csv_filename_y_train)\n",
        "print(f\"File {csv_filename_y_train} siap diunduh.\")\n",
        "\n",
        "print(f\"\\nMengekspor X_test ke {csv_filename_X_test}...\")\n",
        "X_test.to_csv(csv_filename_X_test, index=True)\n",
        "files.download(csv_filename_X_test)\n",
        "print(f\"File {csv_filename_X_test} siap diunduh.\")\n",
        "\n",
        "print(f\"\\nMengekspor y_test ke {csv_filename_y_test}...\")\n",
        "y_test.to_csv(csv_filename_y_test, index=True, header=['Tx_target']) # Tambah header jika y_test adalah Series\n",
        "files.download(csv_filename_y_test)\n",
        "print(f\"File {csv_filename_y_test} siap diunduh.\")"
      ],
      "metadata": {
        "id": "jK8TJ40f9FOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "="
      ],
      "metadata": {
        "id": "aLaY1qHjhpBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Pelatihan Model (Random Forest Regressor) ---\n",
        "print(\"\\n--- Memulai Pelatihan Model (Random Forest Regressor) ---\")\n",
        "\n",
        "# Import library yang dibutuhkan\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# from sklearn.preprocessing import MinMaxScaler # Jika Anda memutuskan untuk menggunakan normalisasi\n",
        "import joblib # Untuk menyimpan model\n",
        "from google.colab import files # Untuk mengunduh file\n",
        "\n",
        "# (Opsional) Normalisasi Fitur (seperti kode sebelumnya)\n",
        "# scaler = MinMaxScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Inisialisasi model Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\\nModel Random Forest Regressor telah diinisialisasi dengan parameter:\")\n",
        "print(rf_model.get_params())\n",
        "\n",
        "# Latih model menggunakan data training\n",
        "print(\"\\nMelatih model...\")\n",
        "model_trained_successfully = False\n",
        "try:\n",
        "    # Jika menggunakan normalisasi:\n",
        "    # rf_model.fit(X_train_scaled, y_train)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    print(\"Model berhasil dilatih.\")\n",
        "    model_trained_successfully = True\n",
        "except Exception as e:\n",
        "    print(f\"Error saat melatih model: {e}\")\n",
        "    print(\"Pastikan X_train dan y_train tidak mengandung nilai NaN dan formatnya benar.\")\n",
        "\n",
        "# (Opsional) Tampilkan Feature Importances (seperti kode sebelumnya)\n",
        "if model_trained_successfully and hasattr(rf_model, 'feature_importances_'):\n",
        "    print(\"\\nFeature Importances:\")\n",
        "    importances = rf_model.feature_importances_\n",
        "    feature_names = X_train.columns\n",
        "    feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
        "    feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
        "\n",
        "    # Visualisasi (opsional, bisa di-skip jika hanya ingin download)\n",
        "    # import matplotlib.pyplot as plt\n",
        "    # import seaborn as sns\n",
        "    # plt.figure(figsize=(12, 8))\n",
        "    # sns.barplot(x='importance', y='feature', data=feature_importance_df.head(20)) # Tampilkan top 20\n",
        "    # plt.title('Top 20 Feature Importances dari Random Forest')\n",
        "    # plt.tight_layout()\n",
        "    # plt.show()\n",
        "    print(feature_importance_df.head(10)) # Tampilkan 10 teratas\n",
        "else:\n",
        "    if model_trained_successfully:\n",
        "        print(\"Tidak dapat mengambil feature importances.\")\n",
        "\n",
        "print(\"\\n--- Pelatihan Model Selesai ---\")\n",
        "\n",
        "# --- Menyimpan dan Mengunduh Model ---\n",
        "if model_trained_successfully:\n",
        "    print(\"\\n--- Menyimpan dan Mengunduh Model ---\")\n",
        "    # Tentukan nama file untuk model\n",
        "    model_filename = 'random_forest_tx_semarang_model.joblib'\n",
        "\n",
        "    # Simpan model yang sudah dilatih menggunakan joblib\n",
        "    # Joblib lebih efisien untuk objek Python besar seperti model scikit-learn\n",
        "    joblib.dump(rf_model, model_filename)\n",
        "    print(f\"\\nModel berhasil disimpan sebagai '{model_filename}' di environment Colab.\")\n",
        "\n",
        "    # (Opsional) Jika Anda menggunakan scaler dan ingin menyimpannya juga:\n",
        "    # scaler_filename = 'minmax_scaler_tx_semarang.joblib'\n",
        "    # joblib.dump(scaler, scaler_filename) # Pastikan 'scaler' adalah variabel scaler Anda\n",
        "    # print(f\"Scaler berhasil disimpan sebagai '{scaler_filename}' di environment Colab.\")\n",
        "    # print(f\"Anda bisa mengunduh scaler dengan: files.download('{scaler_filename}')\")\n",
        "\n",
        "    # Tawarkan untuk mengunduh file model\n",
        "    print(f\"\\nMemulai proses unduh untuk '{model_filename}'...\")\n",
        "    try:\n",
        "        files.download(model_filename)\n",
        "        print(f\"File '{model_filename}' seharusnya mulai diunduh ke browser Anda.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Gagal mengunduh file '{model_filename}': {e}\")\n",
        "        print(\"Anda mungkin perlu mencari file tersebut di panel file Colab (sebelah kiri) dan mengunduhnya secara manual.\")\n",
        "else:\n",
        "    print(\"\\nModel tidak dilatih dengan sukses, jadi tidak ada model untuk disimpan atau diunduh.\")"
      ],
      "metadata": {
        "id": "4eTmnx3jnIT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "="
      ],
      "metadata": {
        "id": "jZI4OydeoBoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Evaluasi Model ---\n",
        "print(\"\\n--- Memulai Evaluasi Model ---\")\n",
        "\n",
        "# Import library metrik evaluasi\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # Untuk visualisasi yang lebih baik\n",
        "\n",
        "# Fungsi untuk menghitung MAPE\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Menghitung Mean Absolute Percentage Error (MAPE).\n",
        "    Menghindari division by zero dengan mengganti y_true == 0 dengan nilai kecil (epsilon)\n",
        "    atau dengan mengabaikan sampel di mana y_true == 0.\n",
        "    Untuk suhu, y_true == 0 sangat jarang, tapi praktik yang baik.\n",
        "    \"\"\"\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "\n",
        "    # Opsi 3: Filter out y_true == 0 (lebih aman untuk MAPE)\n",
        "    mask = y_true != 0\n",
        "    if not np.all(mask):\n",
        "         print(f\"Peringatan: {np.sum(~mask)} nilai aktual adalah nol dan akan diabaikan dalam perhitungan MAPE.\")\n",
        "\n",
        "    if np.sum(mask) == 0: # Jika semua nilai aktual adalah nol\n",
        "        return np.nan # Tidak bisa menghitung MAPE\n",
        "\n",
        "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "\n",
        "\n",
        "if 'rf_model' not in globals() or not hasattr(rf_model, 'predict'):\n",
        "    print(\"Model 'rf_model' belum dilatih atau tidak ditemukan. Jalankan langkah pelatihan terlebih dahulu.\")\n",
        "else:\n",
        "    print(\"\\nMelakukan prediksi pada data test...\")\n",
        "    try:\n",
        "        # Lakukan prediksi pada X_test\n",
        "        # Jika Anda menggunakan normalisasi pada X_test (X_test_scaled), gunakan itu di sini:\n",
        "        # y_pred = rf_model.predict(X_test_scaled)\n",
        "        y_pred = rf_model.predict(X_test)\n",
        "        print(\"Prediksi pada data test berhasil dilakukan.\")\n",
        "\n",
        "        # Hitung metrik evaluasi\n",
        "        print(\"\\nMetrik Evaluasi Model:\")\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse) # Akar dari MSE\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "\n",
        "        print(f\"  Mean Absolute Error (MAE):          {mae:.4f} °C\")\n",
        "        print(f\"  Mean Squared Error (MSE):         {mse:.4f}\")\n",
        "        print(f\"  Root Mean Squared Error (RMSE):   {rmse:.4f} °C\")\n",
        "        print(f\"  R-squared (R²):                   {r2:.4f}\")\n",
        "        if not np.isnan(mape):\n",
        "            print(f\"  Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
        "        else:\n",
        "            print(\"  Mean Absolute Percentage Error (MAPE): Tidak dapat dihitung (mungkin semua y_true adalah 0).\")\n",
        "\n",
        "        # Penjelasan singkat metrik:\n",
        "        # MAE: Rata-rata selisih absolut antara nilai aktual dan prediksi. Semakin kecil semakin baik.\n",
        "        # MSE: Rata-rata kuadrat selisih. Memberikan bobot lebih pada kesalahan besar. Semakin kecil semakin baik.\n",
        "        # RMSE: Akar dari MSE. Dalam unit yang sama dengan target, lebih mudah diinterpretasikan. Semakin kecil semakin baik.\n",
        "        # R²: Proporsi varians dalam variabel dependen yang dapat diprediksi dari variabel independen. Berkisar -inf hingga 1. Semakin dekat ke 1 semakin baik.\n",
        "        # MAPE: Rata-rata persentase selisih absolut. Berguna untuk memahami kesalahan relatif. Semakin kecil semakin baik.\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saat melakukan evaluasi model: {e}\")\n",
        "\n",
        "print(\"\\n--- Evaluasi Model Selesai ---\")"
      ],
      "metadata": {
        "id": "y-ihAW3LM_bS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisasi Hasil Prediksi vs Aktual\n",
        "print(\"\\nVisualisasi Hasil Prediksi vs Aktual pada Data Test...\")\n",
        "\n",
        "# 1. Scatter Plot: Prediksi vs Aktual\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(y_test, y_pred, alpha=0.5, label='Prediksi')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, label='Ideal (y=x)') # Garis diagonal y=x\n",
        "plt.xlabel('Nilai Aktual (Tx °C)')\n",
        "plt.ylabel('Nilai Prediksi (Tx °C)')\n",
        "plt.title('Scatter Plot: Nilai Aktual vs Prediksi (Tx)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0NdUT83YpLwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Plot Garis: Prediksi dan Aktual terhadap Waktu (Indeks Data Test)\n",
        "results_df = pd.DataFrame({'Aktual': y_test, 'Prediksi': y_pred})\n",
        "if isinstance(X_test.index, pd.DatetimeIndex):\n",
        "    results_df.index = X_test.index # Gunakan indeks tanggal dari X_test jika datetime\n",
        "else:\n",
        "    # Jika X_test.index bukan DatetimeIndex, buat range tanggal sederhana untuk plot\n",
        "    # Ini mungkin terjadi jika index direset atau tidak di-set dengan benar\n",
        "    print(\"Peringatan: X_test.index bukan DatetimeIndex. Plot waktu mungkin tidak akurat.\")\n",
        "    results_df.index = pd.date_range(start=pd.Timestamp('today') - pd.Timedelta(days=len(X_test)-1), periods=len(X_test), freq='D')\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "results_df['Aktual'].plot(label='Aktual (Tx)', legend=True)\n",
        "results_df['Prediksi'].plot(label='Prediksi (Tx)', legend=True, linestyle='--')\n",
        "plt.title('Perbandingan Nilai Aktual dan Prediksi Tx pada Data Test')\n",
        "plt.xlabel('Sampel Waktu / Tanggal')\n",
        "plt.ylabel('Temperatur Tx (°C)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fNyj7gVFpNfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Opsional) Plot Distribusi Residual\n",
        "# Residual adalah selisih antara nilai aktual dan prediksi (y_test - y_pred)\n",
        "# Idealnya, residual terdistribusi normal di sekitar nol.\n",
        "\n",
        "residuals = y_test - y_pred\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(residuals, kde=True)\n",
        "plt.title('Distribusi Residual (Aktual - Prediksi)')\n",
        "plt.xlabel('Residual (°C)')\n",
        "plt.ylabel('Frekuensi')\n",
        "plt.axvline(residuals.mean(), color='r', linestyle='dashed', linewidth=1, label=f'Mean Residual: {residuals.mean():.2f}°C')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "print(f\"Rata-rata Residual: {residuals.mean():.4f}°C\")\n",
        "print(f\"Standar Deviasi Residual: {residuals.std():.4f}°C\")"
      ],
      "metadata": {
        "id": "N1x_rchdoBG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "="
      ],
      "metadata": {
        "id": "rGZD1u34p8OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. Prediksi untuk N hari ke depan (Iterative Forecasting) ---\n",
        "print(\"\\n--- Memulai Prediksi untuk N Hari ke Depan (Iterative Forecasting) ---\")\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Fungsi untuk membuat fitur input berdasarkan data hari sebelumnya\n",
        "def create_input_features(last_known_data_series, date_to_predict, feature_columns, lags_to_create):\n",
        "    \"\"\"\n",
        "    Membuat DataFrame dengan satu baris fitur untuk prediksi satu hari.\n",
        "    Args:\n",
        "        last_known_data_series (pd.Series): Series pandas berisi data cuaca terakhir yang diketahui (indeks adalah nama fitur asli seperti 'TX', 'TN').\n",
        "        date_to_predict (datetime): Tanggal yang ingin diprediksi.\n",
        "        feature_columns (pd.Index): Kolom-kolom yang diharapkan oleh model (dari X_train.columns).\n",
        "        lags_to_create (list): Daftar lag yang digunakan saat training (misal [1, 2, 3]).\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame dengan satu baris fitur.\n",
        "    \"\"\"\n",
        "    input_features = pd.Series(index=feature_columns, dtype='float64')\n",
        "\n",
        "    # Isi fitur lag\n",
        "    # Asumsi: last_known_data_series berisi nilai-nilai untuk t-1, t-2, t-3 relatif terhadap 'date_to_predict'\n",
        "    # Ini perlu disesuaikan jika struktur `last_known_data_series` berbeda.\n",
        "    # Untuk implementasi yang lebih robust, `last_known_data_series` bisa berupa DataFrame beberapa hari terakhir.\n",
        "    # Di sini kita sederhanakan, asumsi `last_known_data_series` adalah nilai H-1 dari hari yang akan diprediksi.\n",
        "\n",
        "    # Fitur lag yang dibutuhkan model (misal TX_lag1, TN_lag1, dst.)\n",
        "    # Kolom asli yang menjadi dasar lag (misal TX, TN, dst.)\n",
        "    original_weather_features = ['TN', 'TX', 'TAVG', 'RH_AVG', 'RR', 'SS', 'FF_X', 'DDD_X', 'FF_AVG']\n",
        "\n",
        "    # Untuk lag sederhana ini, kita asumsikan last_known_data_series adalah data H-1\n",
        "    # dan kita hanya mengisi lag1. Untuk lag2, lag3 kita butuh data H-2, H-3\n",
        "    # Ini adalah penyederhanaan. Idealnya, Anda harus mengelola histori beberapa hari terakhir.\n",
        "    for feature_orig in original_weather_features:\n",
        "        for lag in lags_to_create:\n",
        "            lag_col_name = f'{feature_orig}_lag{lag}'\n",
        "            if lag_col_name in input_features.index:\n",
        "                # Ini adalah bagian yang perlu logika lebih canggih\n",
        "                # Untuk demo, kita isi lag1 dari last_known_data_series\n",
        "                # dan lag lainnya bisa NaN atau diisi dari histori yang lebih panjang\n",
        "                if lag == 1 and feature_orig in last_known_data_series.index:\n",
        "                    input_features[lag_col_name] = last_known_data_series[feature_orig]\n",
        "                # else:\n",
        "                    # input_features[lag_col_name] = np.nan # atau ambil dari histori\n",
        "                    # Untuk demo, biarkan NaN jika tidak lag 1. Model akan menangani ini jika dilatih dengan benar\n",
        "                    # atau kita perlu memastikan semua lag terisi.\n",
        "\n",
        "    # Isi fitur berbasis tanggal\n",
        "    input_features['dayofweek'] = date_to_predict.dayofweek\n",
        "    input_features['dayofyear'] = date_to_predict.dayofyear\n",
        "    input_features['month'] = date_to_predict.month\n",
        "    input_features['year'] = date_to_predict.year\n",
        "    # if 'weekofyear' in input_features.index: # Sesuaikan jika Anda menggunakan weekofyear\n",
        "    #     input_features['weekofyear'] = date_to_predict.isocalendar().week\n",
        "\n",
        "    # Mengubah Series menjadi DataFrame dengan satu baris\n",
        "    return pd.DataFrame([input_features])\n",
        "\n",
        "\n",
        "# --- Input dari Pengguna ---\n",
        "# Tanggal mulai prediksi (misalnya hari ini)\n",
        "while True:\n",
        "    start_date_str = input(\"Masukkan tanggal mulai prediksi (YYYY-MM-DD), misal hari ini: \")\n",
        "    try:\n",
        "        start_date_dt = datetime.strptime(start_date_str, '%Y-%m-%d')\n",
        "        break\n",
        "    except ValueError:\n",
        "        print(\"Format tanggal salah. Gunakan YYYY-MM-DD.\")\n",
        "\n",
        "# Jumlah hari ke depan yang ingin diprediksi\n",
        "while True:\n",
        "    try:\n",
        "        days_to_predict_input = int(input(\"Masukkan jumlah hari ke depan yang ingin diprediksi (1-7): \"))\n",
        "        if 1 <= days_to_predict_input <= 7:\n",
        "            break\n",
        "        else:\n",
        "            print(\"Jumlah hari harus antara 1 dan 7.\")\n",
        "    except ValueError:\n",
        "        print(\"Masukkan angka yang valid.\")\n",
        "\n",
        "# --- Persiapan Data Terakhir yang Diketahui ---\n",
        "# Dapatkan data terakhir dari df_processed (data yang sudah bersih dan ada fitur lagnya)\n",
        "# Kita butuh nilai aktual terakhir untuk memulai prediksi iteratif\n",
        "if 'df_processed' not in globals() or df_processed.empty:\n",
        "    print(\"DataFrame 'df_processed' tidak ditemukan atau kosong. Jalankan EDA & Preprocessing.\")\n",
        "else:\n",
        "    # Ambil baris terakhir dari X (fitur input) sebelum target di-drop\n",
        "    # Ini merepresentasikan data H-1 yang fiturnya sudah lengkap (termasuk lag)\n",
        "    # Untuk memulai, kita butuh nilai aktual dari variabel cuaca H-1, H-2, ... H-max_lag\n",
        "    # Untuk penyederhanaan, kita akan coba ambil data aktual terakhir dari df (sebelum feature engineering lag)\n",
        "\n",
        "    # Kita butuh data mentah terakhir untuk mengisi 'last_known_values_for_iteration'\n",
        "    # Ini harus berisi nilai-nilai untuk TN, TX, TAVG, dll. (bukan kolom lag)\n",
        "    # Use df_processed instead of df\n",
        "    last_actual_day_data = df_processed.iloc[-1].copy() # Ambil dari df_processed\n",
        "    print(f\"\\nData aktual terakhir yang digunakan sebagai basis (dari df_processed tanggal {last_actual_day_data.name.strftime('%Y-%m-%d')}):\")\n",
        "    print(last_actual_day_data)\n",
        "\n",
        "    # Kolom fitur yang diharapkan model (dari X_train)\n",
        "    model_feature_columns = X_train.columns\n",
        "    lags_used_in_training = [1, 2, 3] # Sesuaikan dengan yang Anda gunakan di feature engineering\n",
        "\n",
        "# --- Proses Prediksi Iteratif ---\n",
        "future_predictions_tx = []\n",
        "current_date_to_predict = start_date_dt # This is a datetime.datetime object\n",
        "\n",
        "# This is the Series that we will update in each iteration\n",
        "# Initialized with the last actual day's data\n",
        "# The keys are the original feature names (TX, TN, etc.)\n",
        "last_known_values_for_iteration = last_actual_day_data.copy()\n",
        "# Ensure all columns exist, if not in last_actual_day_data, fill with NaN or mean\n",
        "for col in ['TN', 'TX', 'TAVG', 'RH_AVG', 'RR', 'SS', 'FF_X', 'DDD_X', 'FF_AVG']:\n",
        "    if col not in last_known_values_for_iteration:\n",
        "        last_known_values_for_iteration[col] = df_processed[col].mean() # Use df_processed here\n",
        "\n",
        "print(f\"\\nMemulai prediksi iteratif untuk {days_to_predict_input} hari dari {start_date_dt.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "for i in range(days_to_predict_input):\n",
        "    print(f\"\\nMemprediksi untuk hari ke-{i+1}: {current_date_to_predict.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "    # Convert to Pandas Timestamp to access dayofweek, dayofyear, etc.\n",
        "    current_timestamp_to_predict = pd.Timestamp(current_date_to_predict)\n",
        "\n",
        "    # 1. Buat fitur input untuk tanggal saat ini\n",
        "    input_data_dict = {}\n",
        "    # Fitur tanggal\n",
        "    input_data_dict['dayofweek'] = current_timestamp_to_predict.dayofweek\n",
        "    input_data_dict['dayofyear'] = current_timestamp_to_predict.dayofyear\n",
        "    input_data_dict['month'] = current_timestamp_to_predict.month\n",
        "    input_data_dict['year'] = current_timestamp_to_predict.year\n",
        "\n",
        "    # Fitur lag (this is the trickiest part and requires managing history)\n",
        "    # We assume `last_known_values_for_iteration` is H-1\n",
        "    # For lag1:\n",
        "    for feature_orig in ['TN', 'TX', 'TAVG', 'RH_AVG', 'RR', 'SS', 'FF_X', 'DDD_X', 'FF_AVG']:\n",
        "        if f'{feature_orig}_lag1' in model_feature_columns:\n",
        "            input_data_dict[f'{feature_orig}_lag1'] = last_known_values_for_iteration.get(feature_orig, df_processed[feature_orig].mean()) # default to mean if not available\n",
        "\n",
        "    # For lag2, lag3, etc. you need to take from further history\n",
        "    # Example for lag2 (requires data from H-2, which is not directly updated in this loop)\n",
        "    # This is a big simplification and will affect accuracy.\n",
        "    # For the demo, we can try to get it from df_processed if the date matches, or set a default.\n",
        "    # This is the main area for improvement.\n",
        "    # Let's try to build input_df_single_day more explicitly here\n",
        "    # (Keep the explicit construction as you have it)\n",
        "\n",
        "\n",
        "    # Build the DataFrame from the dict, ensuring the column order is the same as X_train\n",
        "    input_df_single_day = pd.DataFrame([input_data_dict], columns=model_feature_columns)\n",
        "\n",
        "    # Fill any remaining NaNs with the mean from the training set (simple strategy)\n",
        "    for col in input_df_single_day.columns:\n",
        "        if input_df_single_day[col].isnull().any():\n",
        "            if col in X_train.columns: # Ensure the column is in the training set\n",
        "                input_df_single_day[col] = input_df_single_day[col].fillna(X_train[col].mean())\n",
        "            else:\n",
        "                input_df_single_day[col] = input_df_single_day[col].fillna(0) # Default if not in training (should not happen)\n",
        "\n",
        "    print(\"Fitur input yang disiapkan untuk prediksi:\")\n",
        "    print(input_df_single_day)\n",
        "\n",
        "    # (Optional) If using a scaler, transform input_df_single_day\n",
        "    # input_df_scaled = scaler.transform(input_df_single_day)\n",
        "    # predicted_tx_value = rf_model.predict(input_df_scaled)[0]\n",
        "\n",
        "    # 2. Perform TX prediction for the current date\n",
        "    try:\n",
        "        predicted_tx_value = rf_model.predict(input_df_single_day)[0]\n",
        "        future_predictions_tx.append({'Tanggal': current_date_to_predict.strftime('%Y-%m-%d'), 'Prediksi_Tx': round(predicted_tx_value, 2)})\n",
        "        print(f\"Prediksi Tx untuk {current_date_to_predict.strftime('%Y-%m-%d')}: {predicted_tx_value:.2f}°C\")\n",
        "\n",
        "        # 3. Update 'last_known_values_for_iteration' with the latest predicted value\n",
        "        # This is IMPORTANT for the next iteration\n",
        "        # Simple assumption: we only update TX.\n",
        "        # Ideally, you also predict TN, TAVG, RH_AVG, etc. and update them.\n",
        "        last_known_values_for_iteration['TX'] = predicted_tx_value\n",
        "\n",
        "        # If you have models for other features, predict and update them here:\n",
        "        # last_known_values_for_iteration['TN'] = predicted_tn_value\n",
        "        # last_known_values_for_iteration['TAVG'] = predicted_tavg_value\n",
        "        # ... and so on\n",
        "\n",
        "        # If no other models, other lagged features will use values from previous iterations or default values.\n",
        "        # For features like RR (rainfall) or SS (sunshine), predicting them is difficult.\n",
        "        # You can use historical average values or 0 as assumptions.\n",
        "        # For example, assume RR and SS are 0 for the next day if there is no model.\n",
        "        last_known_values_for_iteration['RR'] = last_known_values_for_iteration.get('RR_lag1', 0) if 'RR_lag1' in input_df_single_day else 0\n",
        "        last_known_values_for_iteration['SS'] = last_known_values_for_iteration.get('SS_lag1', df_processed['SS'].mean()) if 'SS_lag1' in input_df_single_day else df_processed['SS'].mean()\n",
        "        # And so on for other features required by the `rf_model` as lags\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saat memprediksi untuk {current_date_to_predict.strftime('%Y-%m-%d')}: {e}\")\n",
        "        future_predictions_tx.append({'Tanggal': current_date_to_predict.strftime('%Y-%m-%d'), 'Prediksi_Tx': 'Error'})\n",
        "        # If error, stop further predictions as the next input will be invalid\n",
        "        break\n",
        "\n",
        "    # Move to the next day\n",
        "    current_date_to_predict += timedelta(days=1)\n",
        "\n",
        "# Display the N-day prediction results\n",
        "print(\"\\n--- Hasil Prediksi N Hari ke Depan ---\")\n",
        "if future_predictions_tx:\n",
        "    results_n_days_df = pd.DataFrame(future_predictions_tx)\n",
        "    print(results_n_days_df)\n",
        "else:\n",
        "    print(\"Tidak ada prediksi yang dihasilkan.\")\n",
        "\n",
        "print(\"\\n--- Prediksi Iteratif Selesai ---\")"
      ],
      "metadata": {
        "id": "xPysxa6Ip87r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}